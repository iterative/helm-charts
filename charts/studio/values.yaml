# Default values for studio.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Secret containing Docker registry credentials
imagePullSecrets: []

global:
  # -- Security settings for Bitnami Legacy images
  security:
    # -- Allow insecure images from bitnamilegacy repository
    allowInsecureImages: true

  # -- Studio: Hostname for accessing Studio (no http(s) scheme)
  host: "studio.example.com"
  # -- Studio: Base path (prefix)
  basePath: ""
  # -- Studio: Maximum number of views
  maxViews: "100"
  # -- Studio: Maximum number of teams
  maxTeams: "10"
  # -- Studio: Django SECRET_KEY to encrypt, DB, sign reaquests, etc
  # We recommend you set and manage this externally as other secrets (e.g. DB
  # password, user name, REDIS password, etc).
  # If left empty, a random key will be generated. If it's not saved and lost
  # it might be hard to recover the DB.
  secretKey: ""

  # -- (DEPRECATED) Studio: Custom CA certificate in PEM format
  # Deprecated in favor of `customCaCerts`
  # customCaCert: |-
  #   -----BEGIN CERTIFICATE-----
  #   ....
  #   -----END CERTIFICATE-----
  #
  customCaCert: ""

  # -- Studio: Custom CA certificate in PEM format
  # customCaCerts:
  # - |-
  #     -----BEGIN CERTIFICATE-----
  #     ....
  #     -----END CERTIFICATE-----
  #
  customCaCerts: []


  # -- Studio: Additional environment variables for all pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- Studio: The name of an existing Secret that contains sensitive environment variables
  # passed to all Studio pods.
  envFromSecret: ""

  blobvault:
    # -- Blobvault local backing store settings.
    # Not used when `global.blobvault.bucket` is set.
    persistentVolume:
      # -- Blobvault local backing store access mode.
      # @default -- ReadWriteOnce
      accessModes:
        - ReadWriteOnce

      # -- Blobvault local backing store storage class.
      # @default -- default storage class in the cluster.
      storageClassName: ""

      # -- Blobvault local backing store size.
      size: 30Gi

    # -- Blobvault S3 bucket name
    bucket: ""
    # -- Blobvault S3 endpoint URL
    endpointUrl: ""
    # -- Blobvault S3 access key ID
    accessKeyId: ""
    # -- Blobvault S3 secret access key ID
    secretAccessKeyId: ""
    # -- Blobvault S3 region
    regionName: ""

  celery:
    # -- Celery broker URL
    brokerUrl: ""
    # -- Celery result URL
    resultBackend: ""

  # -- Studio: Settings related to DataChain
  datachain: {}

  ingress:
    enabled: true
    # -- Configure ingress resource to match hostnames to the service
    hostnameEnabled: true
    # -- Ingress class to use
    className: ""
    # -- Additional Ingress annotations
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # -- Expose studio under HTTPS protocol
    tlsEnabled: false
    # -- TLS secret name to use for HTTPS on Ingress
    # For ALB Ingress Controller leave empty.
    tlsSecretName: chart-example-tls

  postgres:
    # -- (DEPRECATED) Postgres database URL
    databaseUrl: ""
    # -- (DEPRECATED) Postgres database user
    databaseUser: ""
    # -- (DEPRECATED) Postgres database password
    databasePassword: ""

    # -- Postgres hostname
    host: "studio-postgresql"
    # -- Postgres port
    port: "5432"
    # -- Postgres database name
    databaseName: "iterativeai"
    # -- Postgres user
    user: "postgres"
    # -- Postgres password
    password: "postgres"

  scmProviders:

    # -- Custom hostname for incoming webhook (if Studio runs on a private network and you use SaaS versions of GitHub, GitLab, or Bitbucket)
    # @default -- `$global.host` value.
    webhookHost: ""

    # -- Enable HTTPS protocol for incoming webhooks (this works only if `global.scmProviders.webhookHost` is set; otherwise is ignored).
    tlsEnabled: false

    # -- GitHub App integration with Studio.
    github:
      # -- GitHub enabled
      enabled: false

      # -- GitHub Enterprise URL
      # Set this if you're using the selfhosted version
      url: ""
      # -- GitHub Enterprise API URL
      # Set this if you're using the selfhosted version
      apiUrl: ""

      # -- GitHub OAuth App Name
      appName: ""
      # -- GitHub OAuth App ID
      appId: ""
      # -- GitHub OAuth App Client ID
      clientId: ""
      # -- GitHub OAuth App Secret
      clientSecret: ""
      # -- GitHub OAuth App Private Key
      privateKey: ""

    # -- GitLab App integration with Studio.
    gitlab:
      # -- GitLab enabled
      enabled: false

      # -- GitLab Enterprise Edition URL
      # Set this if you're using the selfhosted version
      url: ""

      # -- GitLab OAuth App Client ID
      clientId: ""
      # -- GitLab OAuth App Secret Key
      secretKey: ""

      # -- GitLab Webhook Secret
      webhookSecret: ""

    # -- BitBucket App integration with Studio.
    bitbucket:
      # -- Bitbucket enabled
      enabled: false

      # -- Bitbucket Server URL
      # Set this if you're using the selfhosted version
      url: ""
      # -- Bitbucket Server API URL
      # Set this if you're using the selfhosted version
      apiUrl: ""

      # -- Bitbucket OAuth App Client ID
      clientId: ""
      # -- Bitbucket OAuth App Secret Key
      secretKey: ""

redis:
  # -- Redis enabled
  enabled: true
  # -- Redis name override
  fullnameOverride: studio-redis

  # -- Redis image configuration
  image:
    repository: bitnamilegacy/redis

  # -- Redis master configuration
  master:
    ## Redis&reg; master resource requests and limits
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## @param master.resources.limits The resources limits for the Redis&reg; master containers
    ## @param master.resources.requests The requested resources for the Redis&reg; master containers
    resources:
      requests:
        cpu: "200m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
    # -- Redis master persistence configuration
    persistence:
      # -- Redis master persistence is disabled
      enabled: false

  # -- Redis replica configuration
  replica:
    # -- Redis replica count. 0 for standalone deployment of 1 master and 0 replicas
    replicaCount: 0
    # -- Redis replica persistence configuration
    persistence:
      # -- Redis replica persistence is disabled
      enabled: false

  # -- Redis common configuration to be added into the ConfigMap
  commonConfiguration: |-
    timeout 20

  # -- Redis authentication settings
  auth:
    # -- Redis authentication disabled
    enabled: false

postgresql:
  # -- Postgres enabled
  enabled: true
  # -- Postgres name override
  fullnameOverride: studio-postgresql

  # -- Postgres image configuration
  image:
    repository: bitnamilegacy/postgresql
    tag: 14.5.0-debian-11-r35

  # Change this before deploying
  global:
    postgresql:
      auth:
        # -- Postgres password
        postgresPassword: postgres
        # -- Postgres database
        database: iterativeai

clickhouse:
  # -- ClickHouse enabled
  enabled: false
  # -- ClickHouse name override
  fullnameOverride: studio-clickhouse

  # -- ClickHouse image configuration
  image:
    repository: bitnamilegacy/clickhouse

  # Shards / replicas configuration
  replicaCount: 1
  shards: 1

  # Change this before deploying
  auth:
    # -- ClickHouse password
    password: "clickhouse"

# -- PgBouncer settings group
pgBouncer:
  enabled: false

  # -- PgBouncer image settings
  image:
    # -- PgBouncer image repository
    repository: bitnamilegacy/pgbouncer
    # -- PgBouncer image pull policy
    pullPolicy: IfNotPresent
    # -- PgBouncer image tag
    tag: "1.24.1"

  service:
    type: ClusterIP
    port: 6432

  # -- Additional environment variables for PgBouncer pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- The name of an existing Secret that contains sensitive environment variables.
  envFromSecret: ""

  replicaCount: 1

  # -- PgBouncer resources configuration
  resources:
    # -- PgBouncer requests configuration
    requests:
      cpu: 500m
      memory: 512Mi
    # -- PgBouncer limits configuration
    limits:
      memory: 1024Mi

  # -- PgBouncer autoscaling configuration
  autoscaling:
    # -- PgBouncer autoscaling enabled flag
    enabled: false
    # -- PgBouncer autoscaling min replicas
    minReplicas: 1
    # -- PgBouncer autoscaling max replicas
    maxReplicas: 5
    # -- PgBouncer autoscaling target CPU utilization percentage
    targetCPUUtilizationPercentage: 80
    # -- PgBouncer autoscaling target memory utilization percentage
    # targetMemoryUtilizationPercentage: 80

  # -- Additional PgBouncer pod annotations
  podAnnotations: {}

  # -- PgBouncer pod security context configuration
  podSecurityContext: {}
  # fsGroup: 2000

  # -- PgBouncer pod security context configuration
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
  # runAsUser: 1000

  # -- PgBouncer service account name
  serviceAccountName: ""

  # -- PgBouncer pod node selector configuration
  nodeSelector: {}

  # -- PgBouncer pod tolerations configuration
  tolerations: []

  # -- PgBouncer pod affinity configuration
  affinity: {}

# -- Studio UI settings group
studioUi:
  # -- Additional environment variables for ui pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- The name of an existing Secret that contains sensitive environment variables
  # passed to UI pods.
  envFromSecret: ""

  replicaCount: 1

  image:
    repository: docker.iterative.ai/studio-frontend
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    # tag: "v1.34.0"

  service:
    type: ClusterIP
    port: 3000

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      memory: 2Gi

  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  podAnnotations: {}

  podSecurityContext: {}
  # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  nodeSelector: {}

  tolerations: []

  affinity: {}

# -- Studio Backend settings group
studioBackend:
  # -- Additional environment variables for backend pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- The name of an existing Secret that contains sensitive environment variables
  # passed to backend pods.
  envFromSecret: ""

  # -- Number of replicas of backend pods
  replicaCount: 1

  image:
    repository: docker.iterative.ai/studio-backend
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    # tag: "v1.34.0"

  service:
    type: ClusterIP
    port: 8000

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      memory: 2Gi

  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  nodeSelector: {}

  tolerations: []

  affinity: {}

  datachainApi:
    enabled: false

    # -- Number of replicas of Datachain API backend pods
    replicaCount: 1

    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

# -- Studio Beat settings group
studioBeat:
  # -- Additional environment variables for beat pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- The name of an existing Secret that contains sensitive environment variables
  # passed to beat pods.
  envFromSecret: ""

  replicaCount: 1

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      memory: 2Gi

  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  nodeSelector: {}

  tolerations: []

  affinity: {}

# -- Studio worker settings group
studioWorker:
  # -- Studio worker image settings
  image:
    # -- Studio worker image repository
    repository: docker.iterative.ai/studio-backend
    # -- Studio worker image pull policy
    pullPolicy: IfNotPresent
    # -- Overrides the image tag whose default is the chart appVersion.
    # tag: "v1.34.0"

  # -- Additional environment variables for worker pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- The name of an existing Secret that contains sensitive environment variables
  # passed to worker pods.
  envFromSecret: ""

  replicaCount: 1

  # -- Worker log level
  logLevel: "info"

  # -- Worker resources configuration
  resources:
    # -- Worker resource requests configuration
    requests:
      cpu: 500m
      memory: 512Mi
    # -- Worker resource limits configuration
    limits:
      memory: 1Gi

  # -- Worker deployment strategy
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

  # -- Worker termination grace period
  terminationGracePeriodSeconds: 150

  # -- Worker autoscaling configuration
  autoscaling:
    # -- Worker autoscaling enabled flag
    enabled: false
    # -- Worker autoscaling minimum replicas
    minReplicas: 1
    # -- Worker autoscaling maximum replicas
    maxReplicas: 5
    # -- Worker autoscaling target CPU utilization percentage
    # targetCPUUtilizationPercentage: 80
    # -- Worker autoscaling target memory utilization percentage
    # targetMemoryUtilizationPercentage: 80

    # -- Worker autoscaling annotation
    annotations: {}

    # -- Worker Custom or additional autoscaling metrics
    # Custom or additional autoscaling metrics
    # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics
    template: []
    # - type: External
    #   external:
    #     metric:
    #       name: celery-queue-length
    #       selector:
    #         matchLabels:
    #           type: prometheus
    #     target:
    #       type: Value
    #       value: "1"

    # -- Worker autoscaling behavior
    behavior: {}
    # scaleUp:
    #   stabilizationWindowSeconds: 15
    #   policies:
    #     - type: Percent
    #       value: 100
    #       periodSeconds: 1
    #     - type: Pods
    #       value: 2
    #       periodSeconds: 1
    # scaleDown:
    #   stabilizationWindowSeconds: 120
    #   policies:
    #     - type: Pods
    #       value: 1
    #       periodSeconds: 60

  # -- Additional worker pod annotations
  podAnnotations: {}

  # -- Worker pod security context
  podSecurityContext: {}
    # fsGroup: 2000

  # -- Worker security context
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  # -- Worker node selector
  nodeSelector: {}
  # -- Worker tolerations
  tolerations: []
  # -- Worker affinity
  affinity: {}

  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  # -- Ephemeral storage configuration
  ephemeralStorage:
    # -- Ephemeral Storage type. Possible values: `emptyDir`,  `ephemeral`, `pvc`, `pvcRWX`
    type: ephemeral
    # -- Ephemeral Storage size
    size: 1Gi
    # -- Persistent Volume Claim configuration for ephemeral storage (if type is set `pvc`)
    persistentVolumeClaim:
      # -- Persistent Volume Claim name, to mount externally managed PVC (`ephemeralStorage.type` has to be set to `pvc`)
      claimName: "worker"
      # -- Persistent Volume Claim `storageClass` name, by default it will use the default `storageClass`(not used when `pvc` is set as type)
      storageClass: ""


# -- Studio DataChain Worker settings group
studioDatachainWorker:

  # -- DataChain worker image settings
  image:
    # -- DataChain worker image repository
    repository: docker.iterative.ai/studio-datachain-worker
    # -- DataChain worker image pull policy
    pullPolicy: IfNotPresent
    # -- Overrides the image tag whose default is the chart appVersion.
    # tag: "v1.34.0"

  # -- Additional environment variables for DataChain worker pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- The name of an existing Secret that contains sensitive environment variables passed to
  # DataChain worker pods.
  envFromSecret: ""

  replicaCount: 1

  # -- DataChain worker log level
  logLevel: "info"

  # -- DataChain worker resources configuration
  resources:
    # -- DataChain worker requests configuration
    requests:
      cpu: 1000m
      memory: 3Gi
      ephemeral-storage: 10Gi
    # -- DataChain worker limits configuration
    limits:
      memory: 16Gi
      ephemeral-storage: 20Gi

  # -- DataChain worker deployment strategy
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 25%

#  livenessProbe:
#    # -- DataChain worker liveness probe `initialDelaySeconds`
#    initialDelaySeconds: 10
#    # -- DataChain worker liveness probe `periodSeconds`
#    periodSeconds: 10
#    # -- DataChain worker liveness probe `timeoutSeconds`
#    timeoutSeconds: 10

  # -- DataChain worker termination grace period
  terminationGracePeriodSeconds: 180

  # -- DataChain worker autoscaling configuration
  autoscaling:
    # -- DataChain worker autoscaling enabled flag
    enabled: false
    # -- DataChain worker autoscaling min replicas
    minReplicas: 1
    # -- DataChain worker autoscaling max replicas
    maxReplicas: 5
    # -- DataChain worker autoscaling target CPU utilization percentage
    targetCPUUtilizationPercentage: 80
    # -- DataChain worker autoscaling target memory utilization percentage
    # targetMemoryUtilizationPercentage: 80

    # -- Worker autoscaling annotation
    annotations: {}

    # -- DataChain worker Custom or additional autoscaling metrics
    # Custom or additional autoscaling metrics
    # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics
    template: []
    # - type: External
    #   external:
    #     metric:
    #       name: celery-queue-length
    #       selector:
    #         matchLabels:
    #           type: prometheus
    #     target:
    #       type: Value
    #       value: "1"

    # -- DataChain worker autoscaling behavior
    behavior: {}
    # scaleUp:
    #   stabilizationWindowSeconds: 15
    #   policies:
    #     - type: Percent
    #       value: 100
    #       periodSeconds: 1
    #     - type: Pods
    #       value: 2
    #       periodSeconds: 1
    # scaleDown:
    #   stabilizationWindowSeconds: 120
    #   policies:
    #     - type: Pods
    #       value: 1
    #       periodSeconds: 60

  # -- Additional DataChain worker pod annotations
  podAnnotations: {}

  # -- DataChain worker pod security context configuration
  podSecurityContext: {}
    # fsGroup: 2000

  # -- DataChain worker pod security context configuration
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  # -- DataChain worker pod node selector configuration
  nodeSelector: {}

  # -- DataChain worker pod tolerations configuration
  tolerations: []

  # -- DataChain worker pod affinity configuration
  affinity: {}

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

# -- Studio DataChain Worker Job template settings group
studioDatachainWorkerJobTemplate:

  # -- DataChain worker Job image settings
  image:
    # -- DataChain worker Job image repository
    repository: docker.iterative.ai/studio-datachain-worker
    # -- DataChain worker Job image pull policy
    pullPolicy: IfNotPresent
    # -- Overrides the image tag whose default is the chart appVersion.
    # tag: "v1.34.0"

  # -- Additional environment variables for DataChain worker Job pods
  envVars: {}
  # Example:
  # envVars:
  #   DEBUG: "True"

  # -- DataChain worker idle timeout
  idleTimeout: 1800  # 30 minutes

  # -- DataChain worker Job log level
  logLevel: "info"

  # -- DataChain worker Job quota
  jobQuota: 10

  # -- DataChain worker Job resources configuration
  resources:
    # -- DataChain worker Job requests configuration
    requests:
      cpu: 1000m
      memory: 3Gi
      ephemeral-storage: 10Gi
    # -- DataChain worker Job limits configuration
    limits:
      ephemeral-storage: 20Gi

  # -- Ephemeral storage configuration
  ephemeralStorage:
    # -- Ephemeral Storage type. Possible values: `emptyDir`,  `ephemeral`, `pvc`, `pvcRWX`
    type: ephemeral
    # -- Ephemeral Storage size
    size: 20Gi
    # -- Persistent Volume Claim configuration for ephemeral storage (if type is set `pvc`)
    persistentVolumeClaim:
      # -- Persistent Volume Claim name, to mount externally managed PVC (`ephemeralStorage.type` has to be set to `pvc`)
      claimName: "datachain-worker"
      # -- Persistent Volume Claim `storageClass` name, by default it will use the default `storageClass`(not used when `pvc` is set as type)
      storageClass: ""

  # -- Local storage configuration (used for storing DataChain virtual environments)
  localStorage:
    # -- Local Storage type. Possible values: `emptyDir`,  `ephemeral`, `pvc`
    type: ephemeral
    # -- Local Storage size
    size: 50Gi
    # -- Persistent Volume Claim configuration for local storage
    persistentVolumeClaim:
      # -- Persistent Volume Claim name, to mount externally managed PVC (`localStorage.type` has to be set to `pvc`)
      claimName: "datachain-worker-local"
      # -- Persistent Volume Claim `storageClass` name, by default it will use the default `storageClass`(not used when `pvc` is set as type)
      storageClass: ""

#  livenessProbe:
#    # -- DataChain worker Job liveness probe `initialDelaySeconds`
#    initialDelaySeconds: 10
#    # -- DataChain worker Job liveness probe `periodSeconds`
#    periodSeconds: 10
#    # -- DataChain worker Job liveness probe `timeoutSeconds`
#    timeoutSeconds: 10

  # -- DataChain worker Job active deadline (seconds)
  activeDeadlineSeconds: 86400

  # -- DataChain worker Job TTL after finished (seconds)
  ttlSecondsAfterFinished: 30

  # -- DataChain worker Job backoff limit
  backoffLimit: 0

  # -- Additional DataChain worker Job pod annotations
  podAnnotations: {}

  # -- Additional DataChain worker Job pod labels
  podLabels: {}

  # -- DataChain worker Job pod security context configuration
  podSecurityContext: {}
    # fsGroup: 2000

  # -- DataChain worker Job security context configuration
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  # -- DataChain worker Job security context configuration
  runtimeClassName: null  # gvisor

  # -- DataChain worker Job pod node selector configuration
  nodeSelector: {}

  # -- DataChain worker Job pod tolerations configuration
  tolerations: []

  # -- DataChain worker Job pod affinity configuration
  affinity: {}

  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

# -- Studio: Additional service to expose the blobvault files generated by the worker
# It is enabled automatically if the worker is scaled to 1 replica and no bucket is configured
studioBlobvault:

  # -- Image to use for the blobvault service
  image:
    # -- Image repository
    repository: nginx
    # -- Image tag
    tag: 1.28.0-alpine

  service:
    port: 80

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

# -- Studio: Additional service to cache PyPI packages
studioPypiCache:
  # -- PyPI cache enabled
  enabled: false

  # -- Image to use for the pypi-cache service
  image:
    # -- Image repository
    repository: nginx
    # -- Image tag
    tag: 1.27.4-alpine

  service:
    port: 8080

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  # -- Storage configuration
  storage:
    # -- Storage type; either `emptyDir`, `ephemeral`, `pvc` or `pvcRWO`
    type: pvcRWO
    # -- Storage size
    size: 100Gi
    # -- Persistent Volume Claim configuration for storage (if type is set `pvc`)
    persistentVolumeClaim:
      # -- Persistent Volume Claim name, to mount externally managed PVC (`storage.type` has to be set to `pvc`)
      claimName: "pypi-cache"
      # -- Persistent Volume Claim `storageClass` name, by default it will use the default `storageClass`(not used when `pvc` is set as type)
      storageClass: ""

  nodeSelector: {}

  tolerations: []

  affinity: {}

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# -- Vector Agent configuration for log collection (DaemonSet)
vector-agent:
  # -- Vector Agent enabled
  enabled: false
  # -- Vector Agent name override
  fullnameOverride: studio-vector-agent

  # -- Deploy as DaemonSet for log collection from all nodes
  role: Agent

  # -- Vector Agent tolerations
  tolerations:
    - operator: "Exists"  # tolerate all taints

  image:
    # -- The base to use for Vector's image.
    base: alpine

  # -- Vector Agent service account
  serviceAccount:
    create: true
    name: studio-vector-agent

  # -- Vector Agent configuration
  customConfig:
    data_dir: /data/vector
    expire_metrics_secs: 60
    api:
      enabled: false
    sources:
      kubernetes_logs:
        type: kubernetes_logs
        ignore_older_secs: 600
      kubernetes_metrics:
        type: prometheus_scrape
        endpoints:
          - https://${KUBERNETES_NODE_IP}:10250/metrics
        scrape_interval_secs: 30
        auth:
          strategy: bearer
          token: "${KUBERNETES_SERVICE_ACCOUNT_TOKEN:?}"
        tls:
          verify_certificate: false
      kubernetes_metrics_cadvisor:
        type: prometheus_scrape
        endpoints:
          - https://${KUBERNETES_NODE_IP}:10250/metrics/cadvisor
        scrape_interval_secs: 30
        auth:
          strategy: bearer
          token: "${KUBERNETES_SERVICE_ACCOUNT_TOKEN:?}"
        tls:
          verify_certificate: false
    transforms:
      kubernetes_logs_filtered:
        type: remap
        inputs: [kubernetes_logs]
        source: |
          . = {
            "message": .message,
            "source_type": .source_type,
            "stream": .stream,
            "timestamp": .timestamp,
            "kubernetes": {
              "pod_name": .kubernetes.pod_name,
              "namespace": .kubernetes.pod_namespace,
              "container_name": .kubernetes.container_name
            }
          }
      kubernetes_metrics_filtered:
        type: filter
        inputs: [kubernetes_metrics]
        condition: starts_with!(.name, "kubelet_volume_stats_") || .name == "kubelet_image_pull_duration_seconds"
      kubernetes_metrics_cadvisor_filtered:
        type: filter
        inputs: [kubernetes_metrics_cadvisor]
        condition: .name == "node_cpu_usage_seconds_total" || .name == "node_memory_working_set_bytes" || .name == "container_cpu_usage_seconds_total" || .name == "container_memory_working_set_bytes" || .name == "container_start_time_seconds"
    sinks:
      vector_aggregator:
        type: vector
        inputs: [kubernetes_logs_filtered, kubernetes_metrics_filtered, kubernetes_metrics_cadvisor_filtered]
        address: "studio-vector-aggregator:6000"
        compression: true

  env:
    - name: KUBERNETES_SERVICE_ACCOUNT_TOKEN
      valueFrom:
        secretKeyRef:
          name: studio-vector-agent-token
          key: token
    - name: KUBERNETES_NODE_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP

  extraObjects:
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: studio-vector-agent-extended
        annotations:
          helm.sh/hook: pre-install, pre-upgrade
          helm.sh/hook-delete-policy: hook-failed, before-hook-creation

      rules:
        - apiGroups:
            - ""
          resources:
            - nodes/metrics
            - nodes/stats
          verbs:
            - get

    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: studio-vector-agent-extended
        annotations:
          helm.sh/hook: pre-install, pre-upgrade
          helm.sh/hook-delete-policy: hook-failed, before-hook-creation
      subjects:
        - kind: ServiceAccount
          name: studio-vector-agent
          namespace: default  # FIXME: should be configurable
      roleRef:
        kind: ClusterRole
        name: studio-vector-agent-extended

    - apiVersion: v1
      kind: Secret
      metadata:
        name: studio-vector-agent-token
        annotations:
          kubernetes.io/service-account.name: studio-vector-agent
      type: kubernetes.io/service-account-token

  defaultVolumes:
    - name: var-log
      hostPath:
        path: "/var/log/"
  defaultVolumeMounts:
    - name: var-log
      mountPath: "/var/log/"
      readOnly: true

  persistence:
    hostPath:
      enabled: false

# -- Vector Aggregator configuration for log aggregation and processing
vector-aggregator:
  # -- Vector Aggregator enabled
  enabled: false
  # -- Vector Aggregator name override
  fullnameOverride: studio-vector-aggregator

  # -- Deploy as StatefulSet for aggregation and persistence
  role: Aggregator

  # -- Vector Aggregator replica count
  replicaCount: 1

  image:
    # -- The base to use for Vector's image.
    base: alpine

  # -- Vector Aggregator service account
  serviceAccount:
    create: true
    name: studio-vector-aggregator

  # -- Vector Aggregator resources
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi

  # -- Vector Aggregator service configuration
  service:
    enabled: true
    type: ClusterIP
    ports:
      - name: logs
        port: 6000
        targetPort: 6000
        protocol: TCP
      - name: api
        port: 8686
        targetPort: 8686
        protocol: TCP

  # -- Vector Aggregator persistence configuration
  persistence:
    enabled: true
    storageClass: ""
    accessModes: ["ReadWriteOnce"]
    size: 128Gi

  # -- Vector command.
  command: [/bin/ash, -euc]

  # -- Vector arguments.
  args:
    - |-
      ash -euc '
        STORAGE=/data/vector
        MAXIMUM_SIZE_PERCENT=90
        OLDEST_MODIFICATION_DAYS=7

        while sleep 60; do
          echo "Running garbage collection in $STORAGE"

          find "$STORAGE" -type f -mtime +"$OLDEST_MODIFICATION_DAYS" -delete -print |
          while read file; do
            echo "Deleting file because is older than $OLDEST_MODIFICATION_DAYS days: $file"
          done

          until test "$(df "$STORAGE" | awk "NR==2{print \$5}" | tr -d %)" -lt "$MAXIMUM_SIZE_PERCENT"; do
            oldest_file=$(find "$STORAGE" -type f -print0 | xargs -0 -n 128 stat -c "%Y %n" | sort -n | cut -d" " -f2- | head -n1)
            echo "Deleting file because is the oldest and storage is more than $MAXIMUM_SIZE_PERCENT% full: $oldest_file"
            rm "$oldest_file"
          done
        done
      ' & GARBAGECOLLECT=$!

      /usr/local/bin/vector --config-dir /etc/vector/ & VECTOR=$!

      wait -n $VECTOR $GARBAGECOLLECT

  # -- Vector Aggregator configuration
  customConfig:
    data_dir: /data/vector
    expire_metrics_secs: 60
    api:
      enabled: true
      address: 0.0.0.0:8686
      playground: true
    sources:
      vector_agent:
        address: 0.0.0.0:6000
        type: vector
        version: "2"
      kubernetes_events:
        type: http_client
        endpoint: https://kubernetes.default.svc:443/api/v1/events
        scrape_interval_secs: 30
        decoding:
          codec: json
        auth:
          strategy: bearer
          token: "${KUBERNETES_SERVICE_ACCOUNT_TOKEN:?}"
        tls:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        headers:
          Accept:
            - application/json
    transforms:
      kubernetes_events_unnested:
        type: remap
        inputs: [kubernetes_events]
        source: |
          . = unnest!(.items)
      kubernetes_events_normalized:
        type: remap
        inputs: [kubernetes_events_unnested]
        source: |
          . = {
            "run_id": null,
            "node_name": .items.reportingInstance,
            "object_name": .items.involvedObject.name,
            "timestamp": .items.lastTimestamp,
            "message": .items.message,
            "raw": .
          }
      kubernetes_events_deduped:
        type: dedupe
        inputs:
          - kubernetes_events_normalized
        fields:
          match: ["node_name", "object_name", "message", "timestamp"]
      vector_agent_route:
        type: route
        inputs: [vector_agent]
        route:
          logs:
            type: is_log
          metrics:
            type: is_metric
    sinks:
      events_file:
        type: file
        inputs: [kubernetes_events_deduped]
        encoding:
          codec: json
        path: /data/vector/events/%Y-%m-%d.log
      logs_file:
        type: file
        inputs: [vector_agent_route.logs]
        encoding:
          codec: json
        path: /data/vector/logs/%Y-%m-%d-{{ "{{" }} .kubernetes.pod_name {{ "}}" }}.log
      metrics_file:
        type: file
        inputs: [vector_agent_route.metrics]
        encoding:
          codec: json
        path: /data/vector/metrics/%Y-%m-%d.log

  env:
    - name: KUBERNETES_SERVICE_ACCOUNT_TOKEN
      valueFrom:
        secretKeyRef:
          name: studio-vector-aggregator-token
          key: token

  extraObjects:
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: studio-vector-aggregator-extended
        annotations:
          helm.sh/hook: pre-install, pre-upgrade
          helm.sh/hook-delete-policy: hook-failed, before-hook-creation

      rules:
        - apiGroups:
            - ""
          resources:
            - events
          verbs:
            - get
            - list

    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: studio-vector-aggregator-extended
        annotations:
          helm.sh/hook: pre-install, pre-upgrade
          helm.sh/hook-delete-policy: hook-failed, before-hook-creation
      subjects:
        - kind: ServiceAccount
          name: studio-vector-aggregator
          namespace: default  # FIXME: should be configurable
      roleRef:
        kind: ClusterRole
        name: studio-vector-aggregator-extended

    - apiVersion: v1
      kind: Secret
      metadata:
        name: studio-vector-aggregator-token
        annotations:
          kubernetes.io/service-account.name: studio-vector-aggregator
      type: kubernetes.io/service-account-token
